<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Classification | John Ramey]]></title>
  <link href="http://ramhiser.com/blog/categories/classification/atom.xml" rel="self"/>
  <link href="http://ramhiser.com/"/>
  <updated>2013-08-31T11:02:03-07:00</updated>
  <id>http://ramhiser.com/</id>
  <author>
    <name><![CDATA[John Ramey]]></name>
    <email><![CDATA[johnramey@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Brief Look at Mixture Discriminant Analysis]]></title>
    <link href="http://ramhiser.com/blog/2013/07/02/a-brief-look-at-mixture-discriminant-analysis/"/>
    <updated>2013-07-02T10:01:00-07:00</updated>
    <id>http://ramhiser.com/blog/2013/07/02/a-brief-look-at-mixture-discriminant-analysis</id>
    <content type="html"><![CDATA[<p>Lately, I have been working with finite mixture models for my postdoctoral work
on data-driven automated <a href="http://en.wikipedia.org/wiki/Gate_%28cytometry%29">gating</a>.
Given that I had barely scratched the surface with mixture models in the
classroom, I am becoming increasingly comfortable with them. With this in mind,
I wanted to explore their application to classification because there are times
when a single class is clearly made up of multiple subclasses that are not
necessarily adjacent.</p>

<p>As far as I am aware, there are two main approaches (there are lots and lots of
variants!) to applying finite mixture models to classfication:</p>

<ol>
  <li>
    <p>The <a href="http://www.stat.washington.edu/raftery/Research/PDF/fraley2002.pdf">Fraley and Raftery approach</a> via <a href="http://cran.r-project.org/web/packages/mclust/index.html">the mclust R package</a></p>
  </li>
  <li>
    <p>The <a href="http://www.jstor.org/stable/2346171">Hastie and Tibshirani approach</a> via <a href="http://cran.r-project.org/web/packages/mda/index.html">the mda R package</a></p>
  </li>
</ol>

<p>Although the methods are similar, I opted for exploring the latter method. Here
is the general idea. There are <script type="math/tex">K \ge 2</script> classes, and each class is assumed to
be a Gaussian mixuture of subclasses. Hence, the model formulation is generative,
and the posterior probability of class membership is used to classify an
unlabeled observation. Each subclass is assumed to have its own mean vector, but
all subclasses share the same covariance matrix for model parsimony. The model
parameters are estimated via <a href="http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">the EM algorithm</a>.</p>

<p>Because the details of the likelihood in the paper are brief, I realized I was a
bit confused with how to write the likelihood in order to determine how much
each observation contributes to estimating the common covariance matrix in the
M-step of the EM algorithm. Had each subclass had its own covariance matrix, the
likelihood would simply be the product of the individual class likelihoods and
would have been straightforward. The source of my confusion was how to write
the complete data likelihood when the classes share parameters.</p>

<p>I decided to write up a document that explicitly defined the likelihood and
provided the details of the EM algorithm used to estimate the model parameters.
<a href="http://ramhiser.com/research/mixture-discriminant-analysis.html">The document is available here</a>
along with <a href="https://github.com/ramey/tech-reports/tree/master/mixture-discrim-analysis">the LaTeX and R code</a>.
If you are inclined to read the document, please let me know if any notation is
confusing or poorly defined. Note that I did not include the additional topics
on reduced-rank discrimination and shrinkage.</p>

<p>To see how well the mixture discriminant analysis (MDA) model worked, I
constructed a simple toy example consisting of 3 bivariate classes each having 3
subclasses. The subclasses were placed so that within a class, no subclass is
adjacent. The result is that no class is Gaussian. I was interested in seeing
if the MDA classifier could identify the subclasses and also comparing its
decision boundaries with those of <a href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis">linear discriminant analysis (LDA)</a>
and <a href="http://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis">quadratic discriminant analysis (QDA)</a>.
I used the implementation of the LDA and QDA classifiers in <a href="http://cran.r-project.org/web/packages/MASS/index.html">the MASS package</a>.
From the scatterplots and decision boundaries given below,
the LDA and QDA classifiers yielded puzzling decision boundaries as expected.
Contrarily, we can see that the MDA classifier does a good job of identifying
the subclasses. It is important to note that all subclasses in this example have
the same covariance matrix, which caters to the assumption employed in the MDA
classifier. It would be interesting to see how sensitive the classifier is to
deviations from this assumption. Moreover, perhaps a more important investigation
would be to determine how well the MDA classifier performs as the feature
dimension increases relative to the sample size.</p>

<p><img src="http://i.imgur.com/LIQPL0u.png" alt="LDA Decision Boundaries" /></p>

<p><img src="http://i.imgur.com/GeyXCsf.png" alt="QDA Decision Boundaries" /></p>

<p><img src="http://i.imgur.com/lw0iBxe.png" alt="MDA Decision Boundaries" /></p>

<p>``` r Comparison of LDA, QDA, and MDA
library(MASS)
library(mvtnorm)
library(mda)
library(ggplot2)</p>

<p>set.seed(42)
n &lt;- 500</p>

<h1 id="randomly-sample-data">Randomly sample data</h1>
<p>x11 &lt;- rmvnorm(n = n, mean = c(-4, -4))
x12 &lt;- rmvnorm(n = n, mean = c(0, 4))
x13 &lt;- rmvnorm(n = n, mean = c(4, -4))</p>

<p>x21 &lt;- rmvnorm(n = n, mean = c(-4, 4))
x22 &lt;- rmvnorm(n = n, mean = c(4, 4))
x23 &lt;- rmvnorm(n = n, mean = c(0, 0))</p>

<p>x31 &lt;- rmvnorm(n = n, mean = c(-4, 0))
x32 &lt;- rmvnorm(n = n, mean = c(0, -4))
x33 &lt;- rmvnorm(n = n, mean = c(4, 0))</p>

<p>x &lt;- rbind(x11, x12, x13, x21, x22, x23, x31, x32, x33)
train_data &lt;- data.frame(x, y = gl(3, 3 * n))</p>

<h1 id="trains-classifiers">Trains classifiers</h1>
<p>lda_out &lt;- lda(y ~ ., data = train_data)
qda_out &lt;- qda(y ~ ., data = train_data)
mda_out &lt;- mda(y ~ ., data = train_data)</p>

<h1 id="generates-test-data-that-will-be-used-to-generate-the-decision-boundaries-via">Generates test data that will be used to generate the decision boundaries via</h1>
<p># contours
contour_data &lt;- expand.grid(X1 = seq(-8, 8, length = 300),
                            X2 = seq(-8, 8, length = 300))</p>

<h1 id="classifies-the-test-data">Classifies the test data</h1>
<p>lda_predict &lt;- data.frame(contour_data,
                          y = as.numeric(predict(lda_out, contour_data)$class))
qda_predict &lt;- data.frame(contour_data,
                          y = as.numeric(predict(qda_out, contour_data)$class))
mda_predict &lt;- data.frame(contour_data,
                          y = as.numeric(predict(mda_out, contour_data)))</p>

<h1 id="generates-plots">Generates plots</h1>
<p>p &lt;- ggplot(train_data, aes(x = X1, y = X2, color = y)) + geom_point()
p + stat_contour(aes(x = X1, y = X2, z = y), data = lda_predict)
  + ggtitle(“LDA Decision Boundaries”)
p + stat_contour(aes(x = X1, y = X2, z = y), data = qda_predict)
  + ggtitle(“QDA Decision Boundaries”)
p + stat_contour(aes(x = X1, y = X2, z = y), data = mda_predict)
  + ggtitle(“MDA Decision Boundaries”)
```</p>
]]></content>
  </entry>
  
</feed>
