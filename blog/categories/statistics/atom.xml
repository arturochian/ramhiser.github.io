<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Statistics | John Ramey]]></title>
  <link href="http://ramhiser.com/blog/categories/statistics/atom.xml" rel="self"/>
  <link href="http://ramhiser.com/"/>
  <updated>2013-07-02T10:12:47-07:00</updated>
  <id>http://ramhiser.com/</id>
  <author>
    <name><![CDATA[John Ramey]]></name>
    <email><![CDATA[johnramey@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Brief Look at Mixture Discriminant Analysis]]></title>
    <link href="http://ramhiser.com/blog/2013/07/02/a-brief-look-at-mixture-discriminant-analysis/"/>
    <updated>2013-07-02T10:01:00-07:00</updated>
    <id>http://ramhiser.com/blog/2013/07/02/a-brief-look-at-mixture-discriminant-analysis</id>
    <content type="html"><![CDATA[<p>Lately, I have been working with finite mixture models for my postdoctoral work
on data-driven automated <a href="http://en.wikipedia.org/wiki/Gate_%28cytometry%29">gating</a>.
Given that I had barely scratched the surface with mixture models in the
classroom, I am becoming increasingly comfortable with them. With this in mind,
I wanted to explore their application to classification because there are times
when a single class is clearly made up of multiple subclasses that are not
necessarily adjacent.</p>

<p>As far as I am aware, there are two main approaches (there are lots and lots of
variants!) to applying finite mixture models to classfication:</p>

<ol>
  <li>
    <p>The <a href="http://www.stat.washington.edu/raftery/Research/PDF/fraley2002.pdf">Fraley and Raftery approach</a> via <a href="http://cran.r-project.org/web/packages/mclust/index.html">mclust</a></p>
  </li>
  <li>
    <p>The <a href="http://www.jstor.org/stable/2346171">Hastie and Tibshirani approach</a></p>
  </li>
</ol>

<p>Although the methods are similar, I opted for exploring the latter method. Here
is the general idea. There are <script type="math/tex">K \ge 2</script> classes, and each class is assumed to
be a Gaussian mixuture of subclasses. Hence, the model formulation is generative,
and the posterior probability of class membership is used to classify an
unlabeled observation. Each subclass is assumed to have its own mean vector, but
all subclasses share the same covariance matrix for model parsimony. The model
parameters are estimated via <a href="http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">the EM algorithm</a>.</p>

<p>Because the details of the likelihood in the paper are brief, I realized I was a
bit confused with how to write the likelihood in order to determine how much
each observation contributes to estimating the common covariance matrix in the
M-step of the EM algorithm. Had each subclass had its own covariance matrix, the
likelihood would simply be the product of the individual class likelihoods and
would have been straightforward. The source of my confusion was how to write
the complete data likelihood when the classes share parameters.</p>

<p>I decided to write up a document that explicitly defined the likelihood and
provided the details of the EM algorithm used to estimate the model parameters.
<a href="http://ramhiser.com/research/mixture-discriminant-analysis.html">The document is available here</a>
along with <a href="https://github.com/ramey/tech-reports/tree/master/mixture-discrim-analysis">the LaTeX and R code</a>.
If you are inclined to read the document, please let me know if any notation is
confusing or poorly defined.</p>

<p>To see how well the mixture discriminant analysis (MDA) model worked, I
constructed a toy example consisting of 3 bivariate classes each having 3
subclasses. The subclasses were placed so that within a class, no subclass is
adjacent. The result is that no class is Gaussian. I was interested in seeing
if the MDA classifier could identify the subclasses and also comparing its
decision boundaries with those of <a href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis">linear discriminant analysis (LDA)</a>
and <a href="http://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis">quadratic discriminant analysis (QDA)</a>.
I used the implementation of the LDA and QDA classifiers in <a href="http://cran.r-project.org/web/packages/MASS/index.html">the MASS package</a>
and <a href="http://cran.r-project.org/web/packages/mda/index.html">the mda package</a> for
the MDA classifier. From the scatterplots and decision boundaries given below,
the LDA and QDA classifiers yielded puzzling decision boundaries as expected.
Contrarily, we can see that the MDA classifier does a good job of identifying
the subclasses.</p>

<p><img src="http://i.imgur.com/LIQPL0u.png" alt="LDA Decision Boundaries" /></p>

<p><img src="http://i.imgur.com/GeyXCsf.png" alt="QDA Decision Boundaries" /></p>

<p><img src="http://i.imgur.com/lw0iBxe.png" alt="MDA Decision Boundaries" /></p>

<p>``` r Comparison of LDA, QDA, and MDA
library(MASS)
library(mvtnorm)
library(mda)
library(ggplot2)</p>

<p>set.seed(42)
n &lt;- 500</p>

<h1 id="randomly-sample-data">Randomly sample data</h1>
<p>x11 &lt;- rmvnorm(n = n, mean = c(-4, -4))
x12 &lt;- rmvnorm(n = n, mean = c(0, 4))
x13 &lt;- rmvnorm(n = n, mean = c(4, -4))</p>

<p>x21 &lt;- rmvnorm(n = n, mean = c(-4, 4))
x22 &lt;- rmvnorm(n = n, mean = c(4, 4))
x23 &lt;- rmvnorm(n = n, mean = c(0, 0))</p>

<p>x31 &lt;- rmvnorm(n = n, mean = c(-4, 0))
x32 &lt;- rmvnorm(n = n, mean = c(0, -4))
x33 &lt;- rmvnorm(n = n, mean = c(4, 0))</p>

<p>x &lt;- rbind(x11, x12, x13, x21, x22, x23, x31, x32, x33)
train_data &lt;- data.frame(x, y = gl(3, 3 * n))</p>

<h1 id="trains-classifiers">Trains classifiers</h1>
<p>lda_out &lt;- lda(y ~ ., data = train_data)
qda_out &lt;- qda(y ~ ., data = train_data)
mda_out &lt;- mda(y ~ ., data = train_data)</p>

<h1 id="generates-test-data-that-will-be-used-to-generate-the-decision-boundaries-via">Generates test data that will be used to generate the decision boundaries via</h1>
<p># contours
contour_data &lt;- expand.grid(X1 = seq(-8, 8, length = 300),
                            X2 = seq(-8, 8, length = 300))</p>

<h1 id="classifies-the-test-data">Classifies the test data</h1>
<p>lda_predict &lt;- data.frame(contour_data,
                          y = as.numeric(predict(lda_out, contour_data)$class))
qda_predict &lt;- data.frame(contour_data,
                          y = as.numeric(predict(qda_out, contour_data)$class))
mda_predict &lt;- data.frame(contour_data,
                          y = as.numeric(predict(mda_out, contour_data)))</p>

<h1 id="generates-plots">Generates plots</h1>
<p>p &lt;- ggplot(train_data, aes(x = X1, y = X2, color = y)) + geom_point()
p + stat_contour(aes(x = X1, y = X2, z = y), data = lda_predict)
  + ggtitle(“LDA Decision Boundaries”)
p + stat_contour(aes(x = X1, y = X2, z = y), data = qda_predict)
  + ggtitle(“QDA Decision Boundaries”)
p + stat_contour(aes(x = X1, y = X2, z = y), data = mda_predict)
  + ggtitle(“MDA Decision Boundaries”)
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[High-Dimensional Microarray Data Sets in R for Machine Learning]]></title>
    <link href="http://ramhiser.com/blog/2012/12/29/high-dimensional-microarray-data-sets-in-r-for-machine-learning/"/>
    <updated>2012-12-29T14:48:00-08:00</updated>
    <id>http://ramhiser.com/blog/2012/12/29/high-dimensional-microarray-data-sets-in-r-for-machine-learning</id>
    <content type="html"><![CDATA[<p>Much of my research in machine learning is aimed at small-sample, high-dimensional bioinformatics data sets. For instance, here is <a href="http://www.tandfonline.com/doi/full/10.1080/00949655.2011.625946">a paper of mine on the topic</a>.</p>

<p>A large number of papers proposing new machine-learning methods that target high-dimensional data use the same two data sets and consider few others. These data sets are the 1) <a href="https://github.com/ramey/datamicroarray/wiki/Alon-%281999%29">Alon colon cancer data set</a>, and the 2) <a href="https://github.com/ramey/datamicroarray/wiki/Golub-%281999%29">Golub leukemia data set</a>. Both of the corresponding papers were published in 1999, which indicates that the methods are not keeping up with the data-collection techology. Furthermore, the Golub data set is not useful as a benchmark data set because it is well-separated so that most methods have nearly perfect classification.</p>

<p>My goal has been to find several alternative data sets and provide them in a convenient location so that I could load and analyze them easily and then incorporate the results into my papers. Initially, I aimed to identify a few more data sets, but after I got going on this effort, I found a lot more. What started as a small project turned into something that has saved me a lot of time. I have created the <a href="https://github.com/ramey/datamicroarray">datamicroarray package available from my GitHub account</a>. For each data set included in the package, I have provided a script to download, clean, and save the data set as a named list. See the <a href="https://github.com/ramey/datamicroarray/blob/master/README.md">README file</a> for more details about how the data are stored.</p>

<p>Currently, the package consists of 20 small-sample, high-dimensional data sets to assess machine learning algorithms and models. I have also included a <a href="https://github.com/ramey/datamicroarray/wiki">wiki on the package’s GitHub repository</a> that describes each data set and provides additional information, including a link to the original papers.</p>

<p>The biggest drawback at the moment is the file size of the R package because I store an RData file for each data set. I am investigating alternative approaches to download the data dynamically and am open to suggestions. Also note that the data descriptions are incomplete, so assistance is appreciated.</p>

<p>Feel free to use any of the data sets. As a disclaimer, you should ensure that the data are processed correctly before analyzing and incorporating the results into your own work.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Chapter 2 Solutions - Statistical Methods in Bioinformatics]]></title>
    <link href="http://ramhiser.com/blog/2012/08/14/chapter-2-solutions-statistical-methods-in-bioinformatics/"/>
    <updated>2012-08-14T20:42:00-07:00</updated>
    <id>http://ramhiser.com/blog/2012/08/14/chapter-2-solutions-statistical-methods-in-bioinformatics</id>
    <content type="html"><![CDATA[<p>As I have mentioned previously, I have begun reading <a href="http://amzn.to/PiXCiU">Statistical Methods in Bioinformatics by Ewens and Grant</a> and working selected problems for each chapter. In this post, I will give my solution to two problems. The first problem is pretty straightforward.</p>

<h2 id="problem-220">Problem 2.20</h2>
<blockquote>
  <p>Suppose that a parent of genetic type <em>Mm</em> has three children. Then the parent transmits the <em>M</em> gene to each child with probability 1/2, and <strong>the genes that are transmitted to each of the three children are independent</strong>. Let <script type="math/tex">I_1 = 1</script> if children 1 and 2 had the same gene transmitted, and <script type="math/tex">I_1 = 0</script> otherwise. Similarly, let <script type="math/tex">I_2 = 1</script> if children 1 and 3 had the same gene transmitted, <script type="math/tex">I_2 = 0</script> otherwhise, and let <script type="math/tex">I_3 = 1</script> if children 2 and 3 had the same gene transmitted, <script type="math/tex">I_3 = 0</script> otherwise.</p>
</blockquote>

<p>The question first asks us to how that the three random variables are pairwise independent but not independent. The pairwise independence comes directly from the bolded phrase in the problem statement. Now, to show that the three random variables are not independent, denote by <script type="math/tex">p_j</script> the probability that <script type="math/tex">I_j = 1</script>, <script type="math/tex">j = 1, 2, 3</script>. If we had independence, then the following statement would be true:</p>

<script type="math/tex; mode=display">
P(I_1 = 1, I_2 = 1, I_3 = 0) = p_1 p_2 (1 - p_3).
</script>

<p>However, notice that the event in the lefthand side can never happen because if <script type="math/tex">I_1 = 1</script> and <script type="math/tex">I_2 = 1</script>, then <script type="math/tex">I_3</script> must be 1. Hence, the lefthand side must equal 0, while the righthand side equals 1/8. Therefore, the three random variables are not independent.</p>

<p>The question also asks us to discuss why the variance of <script type="math/tex">I_1 + I_2 + I_3</script> is equal to the sum of the individual variances. Often, this is only the case of the random variables are independent. But because the random variables here are pairwise independent, the covariances must be 0. Thus, the equality must hold.</p>

<h2 id="problems-223---227">Problems 2.23 - 2.27</h2>

<p>While I worked the above problem because of its emphasis on genetics, the following set of problems is much more fun in terms of the mathematics because of its usage of approximations.</p>

<blockquote>
  <p>For <script type="math/tex">i = 1, \ldots, n</script>, let <script type="math/tex">X_i</script> be the <script type="math/tex">i</script>th lifetime of certain cellular proteins until degradation. We assume that <script type="math/tex">X_1, \ldots, X_n</script> are iid random variables, each of which is <a href="http://en.wikipedia.org/wiki/Exponential_distribution">exponentially distributed</a> with rate parameter <script type="math/tex">\lambda > 0</script>. Furthermore, let <script type="math/tex">n = 2m + 1</script> be an odd integer.</p>
</blockquote>

<p>This set of questions is concerned with the mean and variance of the sample median, <script type="math/tex">X_{(m + 1)}</script>, where <script type="math/tex">X_{(i)}</script> denotes the <script type="math/tex">i</script>th <a href="http://en.wikipedia.org/wiki/Order_statistic">order statistic</a>. First, note that the mean and variance of the minimum value <script type="math/tex">X_{(1)}</script> are <script type="math/tex">1/(n\lambda)</script> and <script type="math/tex">1/(n\lambda)^2</script>, respectively. From the <a href="http://en.wikipedia.org/wiki/Memorylessness#The_memoryless_distributions_are_the_exponential_distributions">memoryless property</a> of the <a href="http://en.wikipedia.org/wiki/Exponential_distribution">exponential distribution</a>, the mean value of the time until the next protein degrades is independent of the previous. However, there are now <script type="math/tex">n - 1</script> proteins remaining. Thus, the mean and variance of <script type="math/tex">X_{(2)}</script> are <script type="math/tex">1/(n\lambda) + 1/((n-1)\lambda)</script> and <script type="math/tex">1/(n\lambda)^2 + 1/((n-1)\lambda)^2</script>, respectively. Continuining in this manner, we have</p>

<script type="math/tex; mode=display">
E[X_{(m + 1)}] = \frac{1}{(2m + 1)\lambda} + \frac{1}{(2m)\lambda} + \ldots + \frac{1}{(m + 1)\lambda}
</script>

<p>and</p>

<script type="math/tex; mode=display">
Var[X_{(m + 1)}] = \frac{1}{(2m + 1)^2\lambda^2} + \frac{1}{(2m)^2\lambda^2} + \ldots + \frac{1}{(m + 1)^2\lambda^2}.
</script>

<h3 id="approximation-of-exm--1">Approximation of <script type="math/tex">E[X_{(m + 1)}]</script></h3>

<p>Now, we wish to approximate the mean with a much simpler formula. First, from (B.7) in Appendix B, we have</p>

<script type="math/tex; mode=display">
\sum_{k=1}^n \frac{1}{k} \approx \log n + \gamma,
</script>

<p>where <script type="math/tex">\gamma</script> is <a href="http://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant">Euler’s constant</a>. Then, we can write the expected sample median as</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{aligned}
E[X_{(m + 1)}] &= \frac{1}{\lambda} \sum_{k=m+1}^{2m+1} \frac{1}{k}\\
&= \frac{1}{\lambda} \left(\sum_{k=1}^{2m+1} \frac{1}{k} - \sum_{k=1}^{m} \frac{1}{k} \right)\\
&\approx \frac{1}{\lambda} \left( \log (2m + 1) + \gamma - \log m - \gamma \right)\\
&= \frac{1}{\lambda} \log \left(2 + \frac{1}{m} \right).
\end{aligned}
 %]]&gt;</script>

<p>Hence, as <script type="math/tex">n \rightarrow \infty</script>, this approximation goes to <script type="math/tex"> \frac{\log 2}{\lambda}</script>, which is the median of an exponentially distributed random variable. Specifically, the median is the solution to <script type="math/tex">F_X(x) = 1/2</script>, where <script type="math/tex">F_X</script> denotes the <a href="http://en.wikipedia.org/wiki/Cumulative_distribution_function">cumulative distribution function</a> of the random variable <script type="math/tex">X</script>.</p>

<h3 id="improved-approximation-of-exm--1">Improved Approximation of <script type="math/tex">E[X_{(m + 1)}]</script></h3>

<p>It turns out that we can improve this approximation with the following two results:</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{aligned}
\sum_{k=1}^n \frac{1}{k} &= \log n + \frac{1}{2n} + o\left(\frac{1}{n}\right),\\
\log \left(\frac{2m + 1}{m}\right) &= \log 2 + \frac{1}{2m} + o\left(\frac{1}{m}\right).
\end{aligned}
 %]]&gt;</script>

<p>Following the derivation of our above approximation, we have that</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\begin{aligned}
E[X_{(m + 1)}] &= \frac{1}{\lambda} \left(\sum_{k=1}^{2m+1} \frac{1}{k} - \sum_{k=1}^{m} \frac{1}{k} \right)\\
&= \frac{1}{\lambda} \left( \log (2m + 1) + \gamma - \log m - \gamma \right)\\
&= \frac{1}{\lambda} \left[ \log \left( \frac{2m + 1}{m} \right) + \frac{1}{2(2m+1)} - \frac{1}{2m} + o\left(\frac{1}{m}\right)  \right]\\
&= \frac{\log 2}{\lambda} + \frac{1}{2\lambda (2m + 1)} + o\left(\frac{1}{m}\right).
\end{aligned}
 %]]&gt;</script>

<h3 id="approximation-of-varxm--1">Approximation of <script type="math/tex">Var[X_{(m + 1)}]</script></h3>

<p>We can also approximate <script type="math/tex">Var[X_{(m + 1)}]</script> using the approximation</p>

<script type="math/tex; mode=display">
\frac{1}{a^2} + \frac{1}{(a+1)^2} + \ldots + \frac{1}{b^2} \approx \frac{1}{a - 1/2} - \frac{1}{b + 1/2}.
</script>

<p>With <script type="math/tex">a = m+1</script> and <script type="math/tex">b = 2m + 1</script>, we have</p>

<script type="math/tex; mode=display">
Var[X_{(m + 1)}] \approx \frac{2}{\lambda^2} + o\left(\frac{1}{n^2}\right).
</script>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Textbook - Statistical Methods in Bioinformatics]]></title>
    <link href="http://ramhiser.com/blog/2012/08/14/textbook-statistical-methods-in-bioinformatics/"/>
    <updated>2012-08-14T20:17:00-07:00</updated>
    <id>http://ramhiser.com/blog/2012/08/14/textbook-statistical-methods-in-bioinformatics</id>
    <content type="html"><![CDATA[<p>As part of my effort to acquaint myself more with biology, bioinformatics, and statistical genetics, I am trying to find as many resources as I can that provide a solid foundation. For instance, I am wading through <a href="http://amzn.to/Mx5jCm">Molecular Biology of the Cell</a> at a pace of about 10-15 pages per day – this takes nearly an hour every day.</p>

<p>I am also going through <a href="http://amzn.to/PiXCiU">Statistical Methods in Bioinformatics by Ewens and Grant</a> and working selected problems for each chapter. My intention is to post my solutions to these chapter exercises. Thus far, I have made it through the first three chapters, and I will begin posting my solutions soon. I am interested particularly in problems regarding statistical topics with which I have little-to-no experience and also topics where I lack intuition regarding the biological applications.</p>

<p>Here is a thumbnail of the book:</p>

<p><a href="http://amzn.to/PiXCiU"><img src="http://ecx.images-amazon.com/images/I/41UT3fUB%2BKL._BO2,204,203,200_PIsitb-sticker-arrow-click,TopRight,35,-76_AA300_SH20_OU01_.jpg" alt="Statistical Methods in Bioinformatics Textbook" /></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Listing of Statistics and Machine Learning Conferences]]></title>
    <link href="http://ramhiser.com/blog/2011/06/12/listing-of-statistics-and-machine-learning-conferences/"/>
    <updated>2011-06-12T17:20:00-07:00</updated>
    <id>http://ramhiser.com/blog/2011/06/12/listing-of-statistics-and-machine-learning-conferences</id>
    <content type="html"><![CDATA[<p>Occasionally, I will query Google with “statistics conferences”, “machine learning conferences” or “pattern recognition conferences” and the like. But often, it is difficult to obtain anything
meaningful other than the conferences of which I’m already aware (such as JSM, ICML, some IEEE conferences). Today, I found <a href="http://www.wikicfp.com/cfp/">WikiCFP</a>, which is a <strong>A Wiki for Calls for
Papers</strong>. This seems to be what I needed. In particular, the following are very useful to me:</p>

<ul>
  <li><a href="http://www.wikicfp.com/cfp/call?conference=machine%20learning">Machine Learning on WikiCFP</a></li>
  <li><a href="http://www.wikicfp.com/cfp/call?conference=statistics">Statistics on WikiCFP</a></li>
</ul>

<p>It seems limited for statistics though, as JSM is not even listed.</p>
]]></content>
  </entry>
  
</feed>
