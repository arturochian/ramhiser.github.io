
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Research Statement - John Ramey</title>
  <meta name="author" content="John Ramey">

  
  <meta name="description" content="Research Statement Below is a research statement that I wrote in the recent past. Right now, it is lacking an emphasis on bioinformatics, which is &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://ramhiser.com/research/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="John Ramey" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-33406921-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">John Ramey</a></h1>
  
    <h2>Statistics, Machine Learning, and R.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:ramhiser.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="http://twitter.com/ramhiser">@ramhiser</a></li>
  <li><a href="/about/">About</a></li>
  <li><a href="https://github.com/ramey/vitae/raw/master/cv-John_Ramey.pdf">CV</a></li>
  <li><a href="/projects/">Projects</a></li>
  <li><a href="/research/">Research</a></li>
  <li><a href="/blogroll/">Blogroll</a></li>
  <li><a href="/blog/archives/">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article role="article">
  
  <header>
    <h1 class="entry-title">Research Statement</h1>
    
  </header>
  
  <p>Below is a research statement that I wrote in the recent past. Right now, it is lacking an emphasis on bioinformatics, which is the primary source of my applications of machine learning and multivariate statistics. I aim to update the statement soon.</p>

<h2>Introduction</h2>

<p>We are quickly approaching an era in which modern machines and devices will be transitioned to systems that require more automated decisions. I envision that many of these systems will become completely autonomous, while others will depend on persistent human feedback and interaction. In either case, these systems will likely rely on statistical and machine learning models and algorithms, will depend on large, complex data sets, and will present new, unique challenges. Can we model these data sets with traditional methods? How do we implement these methods if an autonomous system requires nearly instantaneous results? How do we know if our selected models will be effective after the system has been deployed? For systems that require human interaction, how can we present intelligible, meaningful results to an expert for feedback without oversimplifying the underlying system?</p>

<p>My research attempts to provide solutions to these questions. Accordingly, I have sought statistical and machine learning research topics that can feasibly be applied to modern data sets and regularly perform well in empirical and theoretical studies. My research interests include topics that emphasize feedback and interaction with domain experts. Furthermore, in my research I have embraced large-scale, high-performance computing (HPC) to improve statistical and machine learning methods in cutting-edge applications rather than merely grappling with them. My passion is in developing cutting-edge models that utilize the latest advancements in computing technology that were considered only in science fiction in previous decades. From my perspective, ample opportunity exists to advance the state-of-the-art in statistical computing and machine learning in practice and to improve the interdisciplinary collaboration between modern statisticians and domain experts.</p>

<p>Specifically, my research interests are clustering evaluation and consensus clustering, naive Bayesian models, active learning, and Quasi-Monte Carlo methods. Below, I discuss my current and ongoing work in these areas.</p>

<h2>Clustering Evaluation and Consensus Clustering</h2>

<p>The evaluation of clustering algorithms is arguably as important as the actual clustering algorithms(see <a href="http://www.ncbi.nlm.nih.gov/pubmed/11301299">Yeung, Haynor, and Ruzzo (2001)</a>), yet assessment methods have not been well-studied and are not as straightforward as the evaluation of supervised learning models. In practice, clusters are generally evaluated via subjective judgment of visualization tools that are prone to oversimplify geometric structure in the data and can be difficult to interpret and misleading. However, as a vital part of exploratory data analysis and initial knowledge discovery, clustering algorithms are often necessary to determine preliminary data patterns and to motivate future modeling assumptions. Hence, preliminary research that depends on fallacious clusters can lead to wasted time and resources as well as invalid preliminary findings.</p>

<p>Nevertheless, we regularly apply a small subset of the large collection of proposed clustering algorithms to a data set in hopes that one of the algorithms will provide insight. Yet how can we trust the determined clusters when any two of the considered clustering algorithms might (strongly) disagree, even in the rare occasion that the true number of clusters is known? If one clustering algorithm yields the most reasonable set of clusters, while the other considered algorithms largely agree on an unreasonable set of clusters, we might have difficulties in establishing that the former algorithm is more reasonable than the latter ones.</p>

<p>Some initial clustering assessment methods have been recently proposed in the bioinformatics literature. These methods typically yield unclear inference, lack probabilistic interpretation, have not been rigorously studied, and cannot handle data sets that have a potentially large number of clusters, as often occurs in peptide identification in the proteomics literature. Additionally, these methods often make unreasonable independence assumptions that yield biased results and invalid conclusions, and they produce scores that are limited to relative comparisons so that efficacy is difficult to establish. I argue that much further clustering evaluation research is needed, and I envision that as researchers further rely on machine learning algorithms in automated systems, we will develop more rigorous clustering assessment methods.</p>

<p>I have developed and will soon publish a practical method to provide a solution to the above issues that are commonly seen in proposed clustering evaluation methods. My proposed method utilizes a subset of <a href="http://www.jstor.org/stable/2334320">decision theoretic admissibility conditions proposed by Fisher and Van Ness</a> and is intended to exploit embarrassingly-parallel computations with a modern HPC system in order to handle data sets with a potentially large number of clusters.</p>

<p>While developing the above method, I realized that the similarity coefficients that are largely used to compare clustering algorithms are equivalent to a likelihood-based method having unreasonable assumptions. From this perspective, I am currently developing a Bayesian method to compare clustering algorithms in a more appropriate manner that has a clear, probabilistic interpretation. This Bayesian approach is closely related to and generalizes the <a href="http://www.broadinstitute.org/cgi-bin/cancer/publications/pub_paper.cgi?mode=view&amp;paper_id=87">consensus clustering method proposed by Monti, Tamayo, Mesirov, and Golub</a> that was developed as an ensemble method of clustering algorithms analogous to committee and ensemble methods in the supervised learning literature. The Bayesian generalization of consensus clustering allows for an aggregation of clustering algorithms. Moreover, not only can individual problematic clusters be identified, but also individual observations about which the considered clustering algorithms disagree can be identified for further analysis to create more robust aggregate clustering.</p>

<h2>Naive Bayesian Models</h2>

<p>The Naive Bayes (NB) classifier is a simple and powerful supervised classification model that is scalable with modern data sets and is extremely fast to compute when combined with modern technology frameworks, such as MapReduce and Hadoop, because the model requires the naive assumption that variables (features) are independent within each class or population. The NB model is typically effective even when the variables are slightly correlated and has been demonstrated to perform well within the natural language processing and bioinformatics literatures. Despite its empirical successes, the NB model assumptions can be problematic and can yield underperforming classification accuracy when features are correlated.</p>

<p>Currently, I am developing improvements for a subfamily of the NB model that corresponds to diagonal covariance matrices by pairing the NB model with methods found in the independent component analysis literature. In particular, my approach corresponds to data transformations that improve the underlying independence assumptions and reduces the dimensionality of the data. When feature vectors are highly correlated, my proposed method can yield significant improvements to classification accuracy. Furthermore, as a byproduct the method is effective in visualizing high-dimensional data that often arise in the bioinformatics literature.</p>

<p>My proposed method necessarily reduces the naivety of the NB model, thereby reducing its computational runtime. To overcome this challenge, I am presently utilizing parallel processing to speed up the calculations and am working to develop approximate solutions to induce significant gains in runtime.</p>

<h2>Regularized Discriminant Analysis</h2>

<p>For a decent introduction to my work on this topic, <a href="http://www.tandfonline.com/doi/abs/10.1080/00949655.2011.625946">check out my JSCS paper with Dr. Phil Young on regularization methods applied to linear discriminant analysis</a>.</p>

<p>More to come&#8230;</p>

<h2>Active Learning</h2>

<p>Relatively few observations in many modern data sets have classification labels for usage with supervised learning methods because the labeling process can be expensive and time-consuming and can require input from a domain expert. When few observations actually have a classification label, the classification performance of routine classifiers can severely degrade, especially when the number of measured variables is large. Methods have been proposed in the semi-supervised learning (SSL) and semi-supervised clustering (SSC) literatures to improve classification accuracy with a large number of unlabeled observations and few labeled observations. Whether supervised learning, SSL, or SSC methods are employed, intuitively, classification accuracy should be improved as we obtain more labels for the unlabeled data by presenting an expert with a subset of well-chosen unlabeled observations to label.</p>

<p><a href="http://www.cs.cmu.edu/~bsettles/publications.html">Settles and Craven</a> have proposed an effective active learning approach known as information density (ID). The authors have stated that the ID method requires a large amount of computing and resorts to ad hoc parameter estimation methods. I will soon publish a paper  demonstrating that the ID method is actually a special case of Bayesian method. In this paper, I have been able to remove the ad hoc estimations and provide probabilistic interpretation of the ID method. Furthermore, because the Bayesian ID method is closely related to <a href="http://jmlr.csail.mit.edu/papers/v1/tipping01a.html">the relevance vector machine model from Tipping</a>, the amount of computation required to construct the ID method can actually be reduced.</p>

<p>My ongoing work in active learning is extending the current methods to include time-dependent features. Currently, the majority of active learning approaches assume that the observations are independent, which can lead to problematic results.</p>

<h2>Quasi-Monte Carlo Methods</h2>

<p>A large portion of my training has been in Bayesian statistics, and although the Bayesian paradigm is philosophically appealing and allows for comprehensible results as probability statements, the amount of computation required by Markov Chain Monte Carlo (MCMC) algorithms is usually quite large. For large-scale data sets, these MCMC methods can generally inhibit the computational performance and practicality of Bayesian modeling; see <a href="http://bayesian.org/sites/default/files/fm/bulletins/1106.pdf">Professor Mike Jordan&#8217;s commentary on <strong>The Era of Big Data</strong></a>.</p>

<p>For Bayesian machine learning models, Quasi-Monte Carlo (QMC) methods should be considered as a possible alternative to MCMC for automated predictions and classifications. Although QMC methods are unable to provide direct estimates of standard errors, they provide a much quicker approach to model estimation, which is necessary for automated systems. Moreover, some of the deterministic (quasi) random number generation methods involved in QMC methods are embarrassingly parallel and can be employed with modern HPC systems. I am currently investigating how to apply QMC methods as an alternative to MCMC methods with Bayesian machine learning methods.</p>

<h2>Conclusion</h2>

<p>As I have discussed, I am largely interested in statistical and machine learning modeling in modern, automated systems that potentially require persistent input and feedback from human experts. My short-term research goals are ambitious and involve the completion of my current work on clustering evaluation, consensus clustering, naive Bayesian models, active learning, and QMC approaches to Bayesian machine learning. I hope to establish myself as an excellent researcher in these areas and more generally in Bayesian approaches to statistical and machine learning. In the long term, I wish to be at an institution that continually produces cutting-edge research and work with talented, experienced individuals while learning from one another. My long-term aim is to continually utilize state-of-the-art technologies to improve the methodologies in machine learning.</p>

  
    <footer>
      
      
    </footer>
  
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2012/08/14/chapter-2-solutions-statistical-methods-in-bioinformatics/">Chapter 2 Solutions - Statistical Methods in Bioinformatics</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/08/14/textbook-statistical-methods-in-bioinformatics/">Textbook - Statistical Methods in Bioinformatics</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/08/11/now-that-we-live-in-seattle/">Now That We Live in Seattle</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/08/04/and-now-i-blog-again/">And Now I Blog Again</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/01/09/goals-for-2012/">Goals for 2012</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating...</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("ramhiser", 4, false);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/ramhiser" class="twitter-follow-button" data-show-count="false">Follow @ramhiser</a>
  
</section>


<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/ramey">@ramey</a> on GitHub
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'ramey',
            count: 5,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>


<section>
  <h1>My Pinboard</h1>
  <ul id="pinboard_linkroll">Fetching linkroll...</ul>
  <p><a href="http://pinboard.in/u:ramhiser">My Pinboard Bookmarks &raquo;</a></p>
</section>
<script type="text/javascript">
  var linkroll = 'pinboard_linkroll'; //id target for pinboard list
  var pinboard_user = "ramhiser"; //id target for pinboard list
  var pinboard_count = 3; //id target for pinboard list
  (function(){
    var pinboardInit = document.createElement('script');
    pinboardInit.type = 'text/javascript';
    pinboardInit.async = true;
    pinboardInit.src = '/javascripts/pinboard.js';
    document.getElementsByTagName('head')[0].appendChild(pinboardInit);
  })();
</script>




  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - John Ramey -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'johnramey';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>





  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
